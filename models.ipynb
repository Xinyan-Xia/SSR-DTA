{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f58f754-d1bb-4eae-b2ca-5c282157d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import einops\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import (GATConv,\n",
    "                                SAGPooling,\n",
    "                                LayerNorm,\n",
    "                                global_mean_pool,\n",
    "                                max_pool_neighbor_x,\n",
    "                                global_add_pool)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "# from params import N_CHEM_NODE_FEAT, N_CHEM_EDGE_FEAT, N_PROT_EDGE_FEAT, N_PROT_NODE_FEAT\n",
    "from sklearn.utils import shuffle\n",
    "from dataset import pygDataSet\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "from metrics import get_metrics_reg\n",
    "from dataset import testFpDataModule\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c7d77-46f7-4c92-9520-f22c95327837",
   "metadata": {
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31c629f-3707-4876-802d-5aee4680fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSI_DDI_Block(nn.Module):\n",
    "    def __init__(self, n_heads, in_features, head_out_feats, ifConv1d = False, window_size = 5, dropout_1 = 0.4, dropout_2 = 0.1, layernorm = True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.in_features = in_features\n",
    "        self.out_features = head_out_feats\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(in_features, n_heads * head_out_feats)\n",
    "        self.ifConv1d = ifConv1d\n",
    "        if self.ifConv1d:\n",
    "            self.conv1d = nn.Conv1d(in_features, in_features, kernel_size=window_size, padding=window_size//2)\n",
    "        self.conv = GATConv(in_features, head_out_feats, n_heads, dropout = dropout_1)\n",
    "        self.readout = SAGPooling(n_heads * head_out_feats, min_score=-1)\n",
    "        if layernorm:\n",
    "            self.norm = LayerNorm(n_heads * head_out_feats)\n",
    "        else:\n",
    "            self.norm = BatchNorm(n_heads * head_out_feats)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_2)\n",
    "    def forward(self, data):\n",
    "        res_x =  self.linear(data.x)\n",
    "        if self.ifConv1d:\n",
    "            data.x = self.conv1d(data.x.t().unsqueeze(0)).squeeze(0).t()\n",
    "        data.x, attention_weights = self.conv(data.x, data.edge_index,return_attention_weights=True)\n",
    "        att_x, att_edge_index, att_edge_attr, att_batch, att_perm, att_scores= self.readout(data.x, data.edge_index, batch=data.batch)\n",
    "        global_graph_emb = global_add_pool(att_x, att_batch)\n",
    "                \n",
    "        data.x = data.x + res_x\n",
    "        data.x = self.relu(self.norm(data.x))\n",
    "        data.x = self.dropout(data.x)\n",
    "        return data, global_graph_emb, attention_weights, att_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6b0085-c4cc-4124-98f9-7e41076c5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAttentionLayer(nn.Module):\n",
    "    def __init__(self, n_features, n1, n2, n3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.h_dim = n_features\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.h_dim * 3, 1024),\n",
    "                                 nn.BatchNorm3d(n1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(1024, 512),\n",
    "                                 nn.BatchNorm3d(n1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(512, 256),\n",
    "                                 nn.BatchNorm3d(n1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(256, 1)\n",
    "                                )\n",
    "        \n",
    "    \n",
    "    def forward(self, v1, v2, v3):\n",
    "        batch_size = v1.shape[0]\n",
    "        c1 = v1.shape[1]\n",
    "        c2 = v2.shape[1]\n",
    "        c3 = v3.shape[1]\n",
    "\n",
    "        \n",
    "        e_activations = torch.cat(\n",
    "            [einops.repeat(v1, 'b c1 h -> b c1 c2 c3 h', c2=c2, c3=c3),\n",
    "            einops.repeat(v2, 'b c2 h -> b c1 c2 c3 h', c1=c1, c3=c3),\n",
    "            einops.repeat(v3, 'b c3 h -> b c1 c2 c3 h', c1=c1, c2=c2)],\n",
    "            dim = -1)\n",
    "        y = self.mlp(e_activations).squeeze(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2a1f25-3f4e-4390-b703-bfeb4e947bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTAProtGraphChemGraph(torch.nn.Module):\n",
    "    def __init__(self, **param_dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.chem_initial_norm = LayerNorm(param_dict[\"chem_in_features\"])\n",
    "        self.prot_initial_norm = LayerNorm(param_dict[\"prot_in_features\"])\n",
    "        \n",
    "        self.chem_blocks = ModuleList()\n",
    "        \n",
    "        chem_in_features = param_dict[\"chem_in_features\"]\n",
    "        prot_in_features = param_dict[\"prot_in_features\"]\n",
    "        \n",
    "\n",
    "        for i, (head_out_feats, n_heads) in enumerate(zip(param_dict[\"chem_heads_out_feat_params\"], param_dict[\"chem_blocks_params\"])):\n",
    "            block = SSI_DDI_Block(n_heads, chem_in_features, head_out_feats, dropout_1=param_dict[\"dropout_1\"], dropout_2=param_dict[\"dropout_2\"])\n",
    "            self.add_module(f\"block{i}\", block)\n",
    "            self.chem_blocks.append(block)\n",
    "            chem_in_features = head_out_feats * n_heads\n",
    "            \n",
    "        self.fp_linear = nn.Sequential(\n",
    "            nn.Linear(2048, chem_in_features),\n",
    "            nn.BatchNorm1d(chem_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.prot_blocks =  ModuleList()\n",
    "#         self.prot_net_norms = ModuleList()\n",
    "        for i, (head_out_feats, n_heads, windows_size) in enumerate(zip(param_dict[\"prot_heads_out_feat_params\"], param_dict[\"prot_blocks_params\"],  param_dict[\"prot_windows_params\"] )):\n",
    "            block = SSI_DDI_Block(n_heads, prot_in_features, head_out_feats, True, windows_size, dropout_1=param_dict[\"dropout_1\"], dropout_2=param_dict[\"dropout_2\"])\n",
    "            self.add_module(f\"block{i}\", block)\n",
    "            self.prot_blocks.append(block)\n",
    "            prot_in_features = head_out_feats * n_heads\n",
    "#             self.prot_net_norms.append(LayerNorm(prot_in_features))\n",
    "            \n",
    "            \n",
    "        self.co_attention = CoAttentionLayer(prot_in_features, len(param_dict[\"chem_blocks_params\"]), len(param_dict[\"prot_blocks_params\"]), 1, dropout=param_dict[\"dropout_3\"])\n",
    "\n",
    "        \n",
    "        self.rel = nn.Parameter(torch.ones(len(param_dict[\"chem_blocks_params\"]), len(param_dict[\"prot_blocks_params\"]), 1)/(len(param_dict[\"chem_blocks_params\"])* len(param_dict[\"prot_blocks_params\"])))\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "    def forward(self, chem_fp, chem_graph, prot_graph):\n",
    "\n",
    "       \n",
    "        chem_graph.x = self.chem_initial_norm(chem_graph.x)\n",
    "        prot_graph.x = self.prot_initial_norm(prot_graph.x)\n",
    "\n",
    "        repr_fp = self.fp_linear(chem_fp).unsqueeze(dim = 1)\n",
    "        \n",
    "        repr_chem = []\n",
    "        repr_prot = []\n",
    "\n",
    "        # new_dict = {}\n",
    "#             new_dict['edge_index'] = attention_weights[0].numpy().tolist()\n",
    "#             new_dict['attention_weights'] = attention_weights[1].numpy().tolist()\n",
    "\n",
    "#             new_dict['att_scores'] = att_scores.numpy().tolist()\n",
    "#             json.dump(new_dict,open(\"output.txt\",\"a+\"))    # 将数据写入json文件中\n",
    "        for i, block in enumerate(self.chem_blocks):\n",
    "            chem_graph, r_chem, attention_weights, att_scores = block(chem_graph)\n",
    "            repr_chem.append(r_chem)\n",
    "#             chem_graph.x = F.elu(self.chem_net_norms[i](chem_graph.x))\n",
    "#             subdict = {}\n",
    "#             subdict['edge_index'] = attention_weights[0]\n",
    "#             subdict['attention_weights'] = attention_weights[1]\n",
    "#             subdict['att_scores'] = att_scores\n",
    "#             new_dict[i] = subdict\n",
    "#         global cIndex\n",
    "#         pickle.dump(new_dict, open(f\"output_data/{str(cIndex)}.pkl\",\"wb\"))  \n",
    "#         cIndex += 1\n",
    "        repr_chem = torch.stack(repr_chem, dim=-2)\n",
    "            \n",
    "        \n",
    "        for i, block in enumerate(self.prot_blocks):\n",
    "            prot_graph, r_prot, attention_weights, att_scores = block(prot_graph)\n",
    "            repr_prot.append(r_prot)\n",
    "#             prot_graph.x = F.elu(self.prot_net_norms[i](prot_graph.x))\n",
    "            \n",
    "        repr_prot = torch.stack(repr_prot, dim=-2)\n",
    "    \n",
    "        \n",
    "        fusion = self.co_attention(repr_chem, repr_prot, repr_fp)\n",
    "        y = fusion * self.rel\n",
    "        y= y.sum(dim=(-1, -2, -3))\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6597f5eb-0249-44c0-bd36-7ce73d9003e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# res = []\n",
    "class testModel(pl.LightningModule):\n",
    "    def __init__(self, **param_dict): \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(param_dict)\n",
    "        \n",
    "        self.model = DTAProtGraphChemGraph(**param_dict)\n",
    "        self.criterion = param_dict[\"criterion\"]\n",
    "        self.lr = param_dict[\"lr\"]\n",
    "        self.batch_size = param_dict[\"batch_size\"]\n",
    "    def forward(self,  chem_fp, ligand_graph, protein_graph):\n",
    "#         print(ligand_graph.device)\n",
    "        pred_y = self.model(chem_fp, ligand_graph, protein_graph)\n",
    "    \n",
    "        return pred_y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y, chem_fp, ligand_graph, protein_graph= batch\n",
    "        pred_y= self(chem_fp, ligand_graph, protein_graph)\n",
    "        \n",
    "        loss = self.criterion(y, pred_y) \n",
    "        \n",
    "        np_pred_y = pred_y.detach().cpu().numpy()\n",
    "        np_y = y.detach().cpu().numpy()\n",
    "        \n",
    "        metrics = get_metrics_reg(np_y, np_pred_y, \"train\")\n",
    "        \n",
    "        metrics[\"loss\"] = loss\n",
    "        \n",
    "        self.log_dict(metrics, batch_size=self.batch_size)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "            \n",
    "        y, chem_fp, ligand_graph, protein_graph = batch\n",
    "        pred_y = self(chem_fp, ligand_graph, protein_graph)\n",
    "        \n",
    "        loss = self.criterion(y, pred_y) \n",
    "\n",
    "        np_pred_y = pred_y.detach().cpu().numpy()\n",
    "        np_y = y.detach().cpu().numpy()\n",
    "\n",
    "        metrics = get_metrics_reg(np_y, np_pred_y, \"valid\", with_rm2=True, with_ci=True)\n",
    "\n",
    "        metrics[\"valid_loss\"] = loss\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar =True, batch_size=self.batch_size)\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "            \n",
    "        y, chem_fp, ligand_graph, protein_graph = batch\n",
    "        pred_y = self(chem_fp, ligand_graph, protein_graph)\n",
    "\n",
    "#         loss = self.criterion(y, pred_y)\n",
    "\n",
    "        np_pred_y = pred_y.detach().cpu().numpy()\n",
    "        np_y = y.detach().cpu().numpy()\n",
    "#         print(np_pred_y - np_y)\n",
    "        global res\n",
    "        res = res + (np_pred_y - np_y).tolist()\n",
    "#         json.dump((np_pred_y - np_y).tolist(), open(\"res.txt\", \"a\"))\n",
    "#         metrics = get_metrics_reg(np_y, np_pred_y, \"test\", with_rm2=True, with_ci=True)\n",
    "\n",
    "#         metrics[\"valid_loss\"] = loss\n",
    "#         print(metrics)\n",
    "        \n",
    "#         self.log_dict(metrics, prog_bar =True, batch_size=self.batch_size)\n",
    "\n",
    "#         return metrics\n",
    "    \n",
    "      \n",
    "    def configure_optimizers(self):\n",
    "        #         weight_decay=\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9e06b-1a63-47c4-8aff-ebdd4e93768d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bc0b3d-c031-4077-a2c2-2aa817665814",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict={\n",
    "    \"chem_in_features\": 23,\n",
    "    \"prot_in_features\": 41,\n",
    "  \n",
    "    \"hidden_dim\" :256,\n",
    "     \"chem_heads_out_feat_params\": [32, 32, 32, 32, 32, 32], \n",
    "    \"chem_blocks_params\": [8, 8, 8, 8, 8, 8],\n",
    "    \"dropout_1\":0.4,\n",
    "    \"dropout_2\":0.1,\n",
    "    \"dropout_2\":0.5,\n",
    "    \"prot_heads_out_feat_params\": [32, 32, 32, 32], \n",
    "    \"prot_blocks_params\": [8, 8, 8, 8],\n",
    "    \"prot_windows_params\": [7, 7, 7, 7],\n",
    "    \"batch_size\": 512,\n",
    "    \"lr\": 5e-4,\n",
    "    \"dataset_name\": \"kiba_cold_drug\",\n",
    "    \"criterion\": nn.MSELoss(),\n",
    "    \"model_name\":\"ssr-dta\",\n",
    "#     \"T_max\": 2000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff54be57-864a-4622-afa7-27ed303d3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = os.getcwd()+'/lightning_logs/checkpoints'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor ='valid_mse',\n",
    "    dirpath = dirpath,\n",
    "    filename = '-{epoch:03d}-{valid_mse:.4f}--{valid_ci:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode= 'min',\n",
    "    save_last=True,\n",
    ")\n",
    "# c_path = \"./lightning_logs/version_32/checkpoints/epoch=36-step=6845.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b9c7b0-28f6-4ab0-9d27-d3b132accd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=[0], max_epochs=2000, check_val_every_n_epoch = 1)\n",
    "\n",
    "\n",
    "trainer.callbacks.append(checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bca72-d44e-41b4-bf17-a69715420834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.\n",
      "  rank_zero_warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /root/autodl-tmp/graphdta/lightning_logs/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                  | Params\n",
      "----------------------------------------------------\n",
      "0 | model     | DTAProtGraphChemGraph | 4.5 M \n",
      "1 | criterion | MSELoss               | 0     \n",
      "----------------------------------------------------\n",
      "4.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.5 M     Total params\n",
      "17.839    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1894: PossibleUserWarning: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6d20579e8c4df6b57e015860daea73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = testModel(**param_dict)\n",
    "dm =  testFpDataModule(**param_dict)\n",
    "\n",
    "trainer.fit(model=model, datamodule=dm)\n",
    "# json.dump(res, open(\"res.txt\", \"w\"))\n",
    "# trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a0c01-8d4f-4397-835d-b720741a1970",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
